---
title: "Statistical Modeling and Classification of Star Types based on Physical Properties"
author: 'SDS348 Spring 2021'
output: html_document
always_allow_html: yes
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F, R.options=list(max.print=100))
```

## Name: Nikita Sidorchuk
### UT EID: nas3428

------------------------------------------------------------------------

### Introduction:

#### The featured dataset of this project is a dataset containing information about various features of stars. The variables included in the dataset are: Temperature, which is a measure of the surface temperature of the star; Luminosity, the amount of light the star emits; Radius, the size of the star; Absolute Magnitude, the brightness of the star accounting for distance (closely related to Luminosity); Star Type, whether it is a dwarf, main sequence, or giant star; Star Color, and Spectral Class, which both categorize the star based on the color of it. The dataset was found on Kaggle.com, created by a user who got much of the information from Wikipedia and derived certain missing values by using specific astrophysics equations. In total, the dataset has 240 observations, and does not need to be made tidy since each row is already an individual observation.
#### I am interested in exploring this dataset because I have an interest in astrophysics and plan to take a couple classes next semester on the subject. The dataset is made to help create a Hertzsprung-Russel diagram, which is a diagram plotting each of the variables against each other to help categorize the stars by their physical properties. From this project, I am interested in seeing which variables have the best potential to predict a stars classification, and which of the variables impact classification most significantly.

##### *Note: Star Type is a categorical variable measured 0-5. There are 6 difference categories of star in this dataset but they cannot be merged together as their features are far too distnict and would make the data skewed.*

```{r}
stars <- read.csv('stars.csv')
```


### EDA:

```{r, layout="l-body-outset"}
library(tidyverse)
library(psych)
library(knitr)
if (!require(kableExtra)) install.packages("kableExtra")

#Create Correlation matrix with bivariate/univariate graphs
stars_numeric <- stars %>%
  select_if(is.numeric)

stars_categorical <- stars %>%
  select(starts_with("Star.type") | starts_with("Spectral")) %>%
  as.data.frame %>%
  mutate(Spectral.Class = factor(Spectral.Class, levels = c("O", "B", "A", "F", "G", "K", "M"))) %>%
  table()

pairs.panels(stars_numeric,
             method = "pearson",
             hist.col = "blue",
             smooth = FALSE, density = FALSE, ellipses = FALSE)

stars_summary <- stars %>%
  group_by(Star.type) %>%
  summarise(mean(Temperature), mean(Luminosity), mean(Radius), mean(Absolute.magnitude))

kable(stars_summary, col.names = c("Star Type", 'Mean Temperature', 'Mean Luminosity', 'Mean Radius', 'Mean Brightness'), digits = 5, caption = "Table 1: Summary of Features of Star Types", align = "c") %>%
  kable_styling(full_width = F)

kable(stars_categorical, caption = "Table 2: Spectral Class Count by Star Type") %>%
  kable_styling(full_width = F)


```

#### Basic exploratory data analysis shows a few important features in the data. Firstly, looking at the main diagonal of the correlation/univariate/bivariate matrix, it can be seen that Temperature, Luminosity, and Radius are highly right skewed in their distribution, while Absolute Magnitude is bimodally distributed. Attempts were made to normalize the data using inverse, ln, log 10, sqrt, exp, and squared transformations but the data remained non-normal after each transform according to the Shaprio-Wilk test. The correlation coefficients reveal that the only highly correlated variables are Absolute Magnitude and Star Type, while the other variables have almost no correlation or a moderate one. Looking at the scatterplots of each variable compared to the other, it can be seen that some of the variables have non-linear relationships, often taking the shape of a logarithmic or an exponential decay distribution.
#### Looking at the tables, several trends can be spotted easily. Firstly, from Table 1, it can be seen that as Star Type changes, the means of the associated physical features also change quite directly. This makes sense, as the Star Type variables represents 0: Brown Dwarf, 1: Red Dwarf, 2: White Dwarf, 3: Main Sequence Star, 4: Supergiant, 5: Hypergiant. As the Star Type value increases, so should the size of the star (which can be seen by the increase in mean radius). Furthermore, brightness also increases with star type, however, the scale for brightness is such that more negative numbers are brighter than more positive ones. Finally, temperature also is consistent with the trend as giant and dwarf stars tend to burn cooler than main sequence ones.

### MANOVA:
### Assumptions:
```{r}
library(car)
library(ggpubr)
if (!require(mvnormtest)) install.packages("mvnormtest")
library(rstatix)

#Covariance Assumption
box_m(stars[, c("Temperature", "Luminosity", "Absolute.magnitude", "Radius")], stars$Star.type)
#p-value = so low it's literally 0 -> assumption not met

#Multivariate Normality
stars %>%
  select(Temperature, Luminosity, Absolute.magnitude, Radius) %>%
  mshapiro_test()
#p-value = 9.27e-17 -> assumption not met

#Multicollinearity
stars %>%
  cor_test(Temperature, Luminosity, Absolute.magnitude, Radius)
#p-values = all very low, assumption met
```

#### For the MANOVA, only the assumptions of multicollinearity, and having more samples than variables were met. Other assumptions such as multivariate normal distribution and groups having the same covariance were not met (p-values for mshapiro and Box's M test are 0 and 9.27e-17, respectively).

####             
### Test:
```{r}
manova_stars <- manova(cbind(Temperature, Luminosity, Radius, Absolute.magnitude) ~ Star.type, data = stars)
summary(manova_stars)
summary.aov(manova_stars)
# All significant, concur with PERMANOVA

library(vegan)
dists <- stars %>%
  select(Temperature, Luminosity, Radius, Absolute.magnitude) %>%
  dist

adonis(dists ~ Star.type, data = stars)
# Results agree, proceed to t tests

pairwise.t.test(stars$Temperature, stars$Star.type, p.adj="none")
pairwise.t.test(stars$Luminosity, stars$Star.type, p.adj="none")
pairwise.t.test(stars$Radius, stars$Star.type, p.adj="none")
pairwise.t.test(stars$Absolute.magnitude, stars$Star.type, p.adj="none")
```
#### 65 tests were conducted: 1 MANOVA, 4 ANOVA, and 60 t-tests. Thus, using the Bonferroni correction, the new p-value for signficance is 0.000769. This also means the probability of at least 1 Type-I error is 0.96435. According to the ANOVA tests, all of the variables have signficantly different means across at least one of the groups. Analyzing the t-tests, most of the variables have several significant differences between one star type and another. For example, across every variable, except Temperature, a star of type 5 (Hypergiant) has a signficant difference from at least 4 other star types. Absolute Magnitude shows a difference across every single star type except 1 and 2 (Red Dwarf and White Dwarf). Luminosity and Temperature seem to be the two variables where there are the least amount of significant differences, with 7 pairs of star types having insignficant p-values.


### Randomization Test:

```{r}
obs_F <- 48.411

Fs <- replicate(5000,{
  # Randomly permute the Temperature across Star Type
  new <- stars %>%
    mutate(Temperature = sample(Temperature))
  # Compute variation within groups
  SSW <- new %>%
    group_by(Star.type) %>%
    summarize(SSW = sum((Temperature - mean(Temperature))^2)) %>%
    summarize(sum(SSW)) %>% 
    pull
  # Compute variation between groups
  SSB <- new %>% 
    mutate(mean = mean(Temperature)) %>%
    group_by(Star.type) %>% 
    mutate(groupmean = mean(Temperature)) %>%
    summarize(SSB = sum((mean - groupmean)^2)) %>%
    summarize(sum(SSB)) %>%
    pull
  # Compute the F-statistic (ratio of MSB and MSW)
  # df for SSB is 6 groups - 1 = 5
  # df for SSW is 240 observations - 6 groups = 234
  (SSB/5)/(SSW/234)
})

hist(Fs, prob = T, main = "Distribution of Sampled F values")
abline(v = obs_F, col="red",add=T)

mean(Fs > obs_F)
```
#### Since many assumptions were not met for the ANOVA tests, a randomization test was conducted to test whether the mean temperature differs between star types. The null hypothesis was that the mean temperature was the same across star types, while the alternative is that it is different between at least 1 star type and the others. The results of the test show that the proportion of randomized F-statistics that are greater than the observed F-statistic is 0, meaning we can reject the null hypothesis and conclude that at least 1 star type's mean temperature is different from the rest.

### Linear Regression Model:

```{r}
# Mean Center Variables
stars <- stars %>%
  mutate(Temperature_C = Temperature - mean(Temperature)) %>%
  mutate(Luminosity_C = Luminosity - mean(Luminosity)) %>%
  mutate(Radius_C = Radius - mean(Radius)) %>%
  mutate(Absolute.magnitude_C = Absolute.magnitude - mean(Absolute.magnitude))

# Change Star.type to categorical variable
stars$Star.type <- as.character(stars$Star.type)

fit_1 <- lm(Temperature ~ Luminosity_C * Absolute.magnitude_C * Star.type, data = stars)
summary(fit_1)

ggplot(stars, aes(x = Absolute.magnitude, y = Temperature)) +
  geom_smooth(aes(col = Star.type), method = 'lm') +
  labs(title = "Star Temperature vs Absolute Magnitude, colored by Star Type",
       x = "Absolute Magnitude",
       y = "Temperature (K)",
       col = "Star Type")

plot(fit_1, which = 1) # Residuals
plot(fit_1, which = 2) # QQ Plot

# Robust Standard Errors
if (!require(sandwich)) install.packages("sandwich")
library(sandwich)
library(lmtest)
coeftest(fit_1, vcov = vcovHC(fit_1))[,1:4]

# Bootstrapping Standard Errors from Observations
# Repeat bootstrapping 5000 times, saving the coefficients each time
samp_SEs <- replicate(5000, {
  # Bootstrap your data (resample observations)
  boot_data <- sample_frac(stars, replace = TRUE)
  # Fit regression model
  fitboot <- lm(Temperature ~ Luminosity_C * Absolute.magnitude_C * Star.type, data = boot_data)
  # Save the coefficients
  coef(fitboot)
})

# Estimated SEs
samp_SEs %>%
  # Transpose the obtained matrices
  t %>%
  # Consider the matrix as a data frame
  as.data.frame %>%
  # Compute the standard error (standard deviation of the sampling distribution)
  summarize_all(sd)
```

#### A linear regression model was constructed to predict star temperature based on luminosity, absolute magnitude, and star type. The model created has an R-squared value of 0.6076, meaning 60.76% of the variance in the model is explained by luminosity, absolute magnitude, and star type, making the model a decent fit and predictor of the variable. The coefficient of mean centered Luminosity is -7.097e-02, with a p-value of 0.1255, making it not a significant predictor, but for every increase in 1 Lo (unit for Luminosity), there is .07097 K decrease in the stars temperature. The coefficient of mean centered Absolute Magnitude says that for every 1 Mv increase in absolute magnitude there is general decrease of 814.0 K decrease in temperature. The Star Type coefficients are the most significant predictors and imply that if a star is classified as a star besides type 0 (Brown Dwarf), the slope of the line increases/decreases by the coefficient amount. The interaction visualization compares a stars type to its absolute magnitude. The coefficients of the interactions between these two variables state that the star classification changes the slope of the absolute magnitude coefficient by the coefficient amount. The graph itself shows a significant difference between star type 0 and star types 2, 3, 4, and 5, which is to be expected as most of the star types have different temepratures and brightnesses, while Brown and Red dwarfs are fairly similar.
#### None of the assumptions for the linear regression were not met in conducting the model. From the correlation matrix it was seen that few of the variables had a linear correlation, while the QQ plot shows an obvious lack of normality, and the residual plot shows heteroskedacsiticity, due to the cone shape of the residual spread. Thus, robust standard errors and bootstrapping methods were used to recompute similar parameters. Robust SEs estimated coefficients to have p-values where only 4 coefficients were significant (Star types 2 and 5, Absolute.magnitude:Star.type2/5), compared to 7 from the original regression model. The coefficients more or less remained fairly similar, though the Robust SEs tend to be slightly more negative. Bootstrapped SEs that were produced differ quite drastically from the original regression model, as most of the coefficients are 1 to 3 orders of magnitude greater than the original regression model; the non-interaction coefficients tend to be much larger, while the interaction coefficients are much closer to 0.

### Logistic Regression Model:

```{r}
# Create binary categorical variable based on whether the star is a main sequence or not
stars <- stars %>%
  mutate(hypergiant = ifelse(Star.type == 5, 1, 0))

# Create model
model_a <- glm(hypergiant ~ Temperature + Luminosity, data = stars, family = binomial(link="logit"))
summary(model_a)

stars$prob <- predict(model_a, type = "response")
stars$predicted <- ifelse(stars$prob > .5, "Hypergiant", "Other") 

# Confusion Matrix
table(truth = stars$hypergiant, prediction = stars$predicted) %>% 
  addmargins

# Plot
ggplot(stars, aes(Luminosity, hypergiant)) +
  geom_jitter(aes(color = predicted), width = .3, height = 0) +
  stat_smooth(method="glm", method.args = list(family="binomial"), se = FALSE) +
  geom_hline(yintercept = 0.5, lty = 2) +
  ylab("Pr(Hypergiant)")

# Save the predicted log-odds in the dataset
stars$logit <- predict(model_a)

# Compare to the outcome in the dataset with a density plot
ggplot(stars, aes(logit, fill = as.factor(hypergiant))) +
  geom_density(alpha = .3) +
  geom_vline(xintercept = 0, lty = 2) +
  labs(fill = "Hypergiant")

#Accuracy = 83.75%
(10 + 191) / 240

#Sensitivity (TPR) = 25%
10 / 40

#Specificity (TNR) = 95.5%
191 / 200

# Recall/Precision (PPV) = 52.63%
10 / 19

# ROC Plot
library(plotROC)
ROCplot1 <- ggplot(stars) + 
  geom_roc(aes(d = hypergiant, m = prob), cutoffs.at = list(0.1, 0.5, 0.9))
ROCplot1

# AUC
calc_auc(ROCplot1)
```

#### A logistic regression model was created to predict whether a star was a Hypergiant from its temperature and luminosity. The coeffecients of Temperature and Luminosity were -5.874e-05 and 7.790e-06, respectively. These mean that, while holding other coefficients constant, for every 1 K increase in temperature, there is a -5.874e-05 decrease in the log odds of the star being a Hypergiant, similarly, for every 1 Lo increase in luminosity there is a 7.790e-06 increase in the star being a Hypergiant. While these may seem like very small increases, it makes sense due to the massive scales that temperature and luminosity are measured on. Looking at the ROC plot, it can be seen that a 0.5 cut off for the probability is not ideal since it only gives an approximalte 0.25 true positive rate and a 0.05 false positive rate. A better cutoff rate would be 0.1 since its true positve rate is nearly 1, and the false positive rate is only 0.13. The overall AUC for the ROC is 0.9121, meaning the model is a fairly good predictor.








